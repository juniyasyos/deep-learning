{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94aeecc1",
   "metadata": {},
   "source": [
    "# Praktikum Deep Learning — Transfer Learning (Pengantar & Demo Konsep)\n",
    "\n",
    "_Notebook ini memandu praktikum singkat untuk memahami konsep inti transfer learning sebelum studi kasus medis._\n",
    "\n",
    "> Gunakan menu **View → Table of Contents** di JupyterLab atau ekstensi serupa agar heading otomatis tersusun sebagai ToC.\n",
    "\n",
    "## Daftar Isi\n",
    "- A. Judul & Tujuan Pembelajaran\n",
    "- B. Recall & Icebreaker\n",
    "- C. Ringkasan Teori\n",
    "- D. Setup Lingkungan\n",
    "- E. Konfigurasi (Code)\n",
    "- F. Template DataModule (Code)\n",
    "- G. Bangun Model Pretrained (Code)\n",
    "- H. Loop Train Generic (Code)\n",
    "- I. Plot & Logging (Code)\n",
    "- J. Ringkasan & Diskusi\n",
    "- K. Checklist Praktikum\n",
    "- L. Referensi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2e197",
   "metadata": {},
   "source": [
    "## A. Judul & Tujuan Pembelajaran\n",
    "\n",
    "**Learning Objectives**\n",
    "- Memahami konsep Transfer Learning dan motivasinya (hemat data, waktu, sumber daya).\n",
    "- Memahami kelebihan/manfaat utama (efisiensi, mengurangi kebutuhan data, potensi performa lebih baik, mitigasi overfitting).\n",
    "- Mengenali jenis-jenis Transfer Learning (Inductive, Transductive, Unsupervised).\n",
    "- Membedakan feature extraction (freeze) vs fine-tuning dan kapan menggunakannya.\n",
    "- Menyiapkan template eksperimen (tanpa dataset dahulu) yang bisa dipakai ulang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3d316",
   "metadata": {},
   "source": [
    "## B. Recall & Icebreaker\n",
    "\n",
    "- Apa tantangan training CNN dari nol dengan data kecil?\n",
    "- Kenapa reuse knowledge dari model besar itu masuk akal?\n",
    "- Kapan cukup freeze? Kapan perlu fine-tune beberapa layer terakhir?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20cfc1",
   "metadata": {},
   "source": [
    "## C. Ringkasan Teori\n",
    "\n",
    "Transfer learning adalah pendekatan reuse model terlatih sebagai titik awal agar hemat komputasi, data, dan waktu; kita tidak selalu harus memulai training dari nol.\n",
    "\n",
    "**Manfaat & Kelebihan:** efisiensi training, kebutuhan data lebih sedikit, potensi performa lebih baik, dan membantu mengurangi risiko overfitting pada dataset kecil.\n",
    "\n",
    "**Jenis-jenis Transfer Learning:**\n",
    "1. *Inductive* — data target berlabel dan memiliki tugas berbeda, model sumber memberi prior pengetahuan (contoh: ImageNet → klasifikasi medis).\n",
    "2. *Transductive* — tugas sama tetapi distribusi data berbeda, sering muncul saat domain shift.\n",
    "3. *Unsupervised* — memanfaatkan representasi yang dipelajari tanpa label ke domain baru yang juga tidak berlabel.\n",
    "\n",
    "**Fine-tuning vs Freeze:** Feature extraction (freeze) cepat karena hanya melatih kepala baru, cocok saat dataset kecil atau mirip dengan domain sumber. Fine-tuning membuka sebagian/seluruh backbone untuk menyesuaikan representasi, memberikan fleksibilitas lebih tetapi butuh data lebih banyak dan kontrol regularisasi untuk menghindari overfitting.\n",
    "\n",
    "**Use cases populer:** Computer Vision (ImageNet → domain khusus seperti medis atau industri), NLP (BERT/GPT untuk berbagai downstream task), Speech (ASR pretrained → dialek baru).\n",
    "\n",
    "> Catatan: Materi ini diselaraskan dengan slide **“Deep Learning 06 — Transfer Learning”** untuk TM; gunakan slide tersebut sebagai referensi narasi lengkap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869200f9",
   "metadata": {},
   "source": [
    "## D. Setup Lingkungan\n",
    "\n",
    "Inisialisasi dependensi, cek versi Python/GPU, set seed deterministik, dan pastikan direktori output siap sebelum eksperimen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da24868a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, optim\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import functional as F_transforms\n",
    "import yaml\n",
    "\n",
    "# Fungsi utilitas agar hasil eksperimen deterministik untuk kebutuhan praktikum.\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set random seeds untuk numpy, random, dan torch agar hasil reproducible.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_device(preference: str = \"cuda_if_available\") -> torch.device:\n",
    "    \"\"\"Pilih device berdasarkan preferensi dan ketersediaan CUDA.\"\"\"\n",
    "    if preference == \"cuda_if_available\" and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# Deteksi root project (notebook berada di folder notebooks/).\n",
    "project_root = Path.cwd().resolve()\n",
    "if project_root.name == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "paths_to_create = [\n",
    "    project_root / \"outputs\" / \"figures\",\n",
    "    project_root / \"outputs\" / \"reports\",\n",
    "    project_root / \"models\",\n",
    "]\n",
    "for path in paths_to_create:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Simpan cache weight torchvision ke folder models/ agar rapih (dan reusable jika tersedia).\n",
    "os.environ.setdefault(\"TORCH_HOME\", str(project_root / \"models\"))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "seed_value = 42\n",
    "set_seed(seed_value)\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Seed set to: {seed_value}\")\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0b76a",
   "metadata": {},
   "source": [
    "## E. Konfigurasi (Code)\n",
    "\n",
    "Memuat file YAML konfigurasi eksperimen, menampilkannya sebagai tabel, serta menyesuaikan seed dan device sesuai preferensi praktikum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca konfigurasi template agar eksperimen mudah direplikasi.\n",
    "config_path = project_root / \"configs\" / \"training.yaml\"\n",
    "with config_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Menormalkan nilai num_workers agar aman dijalankan lintas OS / notebook.\n",
    "num_workers_cfg = int(config.get(\"num_workers\", 0))\n",
    "if os.name == \"nt\":\n",
    "    num_workers_cfg = 0  # Windows + notebook lebih stabil single-thread loader.\n",
    "if num_workers_cfg < 0:\n",
    "    num_workers_cfg = 0\n",
    "config[\"num_workers\"] = num_workers_cfg\n",
    "\n",
    "config_df = pd.DataFrame(list(config.items()), columns=[\"parameter\", \"value\"]).set_index(\"parameter\")\n",
    "display(config_df)\n",
    "\n",
    "seed_value = config.get(\"seed\", seed_value)\n",
    "set_seed(seed_value)\n",
    "device = get_device(config.get(\"device\", \"cuda_if_available\"))\n",
    "print(f\"Konfigurasi dimuat dari: {config_path}\")\n",
    "print(f\"Seed aktif: {seed_value}\")\n",
    "print(f\"Device aktif: {device}\")\n",
    "print(f\"num_workers efektif: {config['num_workers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921e8ed",
   "metadata": {},
   "source": [
    "## F. Template DataModule (Code)\n",
    "\n",
    "Template DataModule sederhana berbasis dummy tensor untuk mendemokan alur. **TODO:** ganti dengan `torchvision.datasets.ImageFolder` atau dataset medis pada sesi studi kasus berikutnya. Transformasi menggunakan standar ImageNet agar konsisten dengan backbone pretrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset dummy agar loop training dapat dijalankan tanpa dataset eksternal.\n",
    "class DummyRandomDataset(Dataset):\n",
    "    \"\"\"Membuat sampel RGB acak dan label dummy untuk simulasi pipeline.\"\"\"\n",
    "    def __init__(self, num_samples: int, num_classes: int, transform=None, seed: int = 42) -> None:\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "        gen = torch.Generator().manual_seed(seed)\n",
    "        self.images = torch.rand(num_samples, 3, 256, 256, generator=gen)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,), generator=gen)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        pil_image = F_transforms.to_pil_image(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(pil_image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(pil_image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class SimpleImageDataModule:\n",
    "    \"\"\"Kerangka DataModule minimal untuk praktikum transfer learning.\"\"\"\n",
    "    def __init__(self, batch_size: int, num_workers: int, num_classes: int = 2, seed: int = 42) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = max(0, int(num_workers))\n",
    "        if os.name == \"nt\":\n",
    "            self.num_workers = 0  # Hindari multiprocessing issue pada Windows notebooks.\n",
    "        self.num_classes = num_classes\n",
    "        self.seed = seed\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.val_transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        \"\"\"Membuat dataset dummy untuk train/val agar loop model dapat dieksekusi.\"\"\"\n",
    "        self.train_dataset = DummyRandomDataset(\n",
    "            num_samples=32,\n",
    "            num_classes=self.num_classes,\n",
    "            transform=self.train_transforms,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "        self.val_dataset = DummyRandomDataset(\n",
    "            num_samples=16,\n",
    "            num_classes=self.num_classes,\n",
    "            transform=self.val_transforms,\n",
    "            seed=self.seed + 1,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2  # Ubah saat dataset sebenarnya tersedia.\n",
    "datamodule = SimpleImageDataModule(\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    num_classes=NUM_CLASSES,\n",
    "    seed=seed_value,\n",
    ")\n",
    "datamodule.setup()\n",
    "train_batch = next(iter(datamodule.train_dataloader()))\n",
    "print(f\"Contoh batch train: images {train_batch[0].shape}, labels {train_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525d1ea",
   "metadata": {},
   "source": [
    "## G. Bangun Model Pretrained (Code)\n",
    "\n",
    "Mengambil backbone pretrained (`resnet18`), memisahkan feature extractor vs classifier, serta menyiapkan fungsi freeze/unfreeze untuk mode feature extraction dan fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitas pemodelan untuk memisahkan backbone dan classifier.\n",
    "def build_backbone(name: str = \"resnet18\", pretrained: bool = True, num_classes: int = NUM_CLASSES):\n",
    "    \"\"\"Membangun backbone torchvision dan memisahkan classifier head.\"\"\"\n",
    "    if name != \"resnet18\":\n",
    "        raise ValueError(\"Demo ini saat ini hanya mendukung resnet18 sebagai baseline ringan.\")\n",
    "\n",
    "    base_model = None\n",
    "    weights_info = \"random-init\"\n",
    "    if pretrained:\n",
    "        try:\n",
    "            if hasattr(models, \"ResNet18_Weights\"):\n",
    "                base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "                weights_info = \"ResNet18_Weights.DEFAULT\"\n",
    "            else:\n",
    "                base_model = models.resnet18(pretrained=True)\n",
    "                weights_info = \"pretrained=True\"\n",
    "        except Exception as exc:\n",
    "            print(f\"Gagal memuat weight pretrained (offline?): {exc}\")\n",
    "            weights_info = \"random-init (fallback)\"\n",
    "\n",
    "    if base_model is None:\n",
    "        if hasattr(models, \"ResNet18_Weights\"):\n",
    "            base_model = models.resnet18(weights=None)\n",
    "        else:\n",
    "            base_model = models.resnet18(pretrained=False)\n",
    "\n",
    "    feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n",
    "    in_features = base_model.fc.in_features\n",
    "    classifier = nn.Linear(in_features, num_classes)\n",
    "    return feature_extractor, classifier, weights_info\n",
    "\n",
    "\n",
    "class TransferLearner(nn.Module):\n",
    "    \"\"\"Model wrapper yang memisahkan feature extractor dan classifier.\"\"\"\n",
    "    def __init__(self, feature_extractor: nn.Module, classifier: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature_extractor(x)\n",
    "        features = torch.flatten(features, 1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "def set_feature_extractor_grad(feature_extractor: nn.Module, freeze_until: str = \"all\") -> None:\n",
    "    \"\"\"Atur parameter backbone yang dapat di-train sesuai kebijakan freeze.\"\"\"\n",
    "    freeze_until = freeze_until.lower()\n",
    "    if freeze_until not in {\"all\", \"layer4\", \"none\"}:\n",
    "        raise ValueError(\"freeze_until harus salah satu dari: all | layer4 | none\")\n",
    "\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if freeze_until == \"layer4\":\n",
    "        # Layer ke-7 pada sequential merupakan block layer4 pada ResNet18.\n",
    "        for name, module in feature_extractor.named_children():\n",
    "            if name == \"7\":\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True\n",
    "    elif freeze_until == \"none\":\n",
    "        for param in feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "feature_extractor, classifier, weights_info = build_backbone(\n",
    "    name=config.get(\"pretrained_backbone\", \"resnet18\"),\n",
    "    pretrained=True,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "print(f\"Backbone weight source: {weights_info}\")\n",
    "model = TransferLearner(feature_extractor, classifier).to(device)\n",
    "\n",
    "# Mode feature extraction: seluruh backbone di-freeze.\n",
    "set_feature_extractor_grad(model.feature_extractor, freeze_until=\"all\")\n",
    "fe_params = count_trainable_parameters(model)\n",
    "print(f\"Trainable params (feature extraction): {fe_params}\")\n",
    "\n",
    "# Mode fine-tuning: buka sesuai konfigurasi freeze_until.\n",
    "set_feature_extractor_grad(model.feature_extractor, freeze_until=config.get(\"freeze_until\", \"all\"))\n",
    "ft_params = count_trainable_parameters(model)\n",
    "print(f\"Trainable params (fine-tuning policy '{config.get('freeze_until', 'all')}'): {ft_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73368c8",
   "metadata": {},
   "source": [
    "## H. Loop Train Generic (Code)\n",
    "\n",
    "Loop training minimalis untuk mendemokan dua tahap: feature extraction (melatih classifier saja) dan fine-tuning (opsional membuka sebagian backbone). Logging bersifat sederhana karena dataset masih dummy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi training sederhana agar pipeline dapat dijalankan end-to-end.\n",
    "def train_one_epoch(model: nn.Module, dataloader: DataLoader, criterion, optimizer, device: torch.device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / max(total, 1)\n",
    "    epoch_acc = correct / max(total, 1)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, criterion, device: torch.device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    epoch_loss = running_loss / max(total, 1)\n",
    "    epoch_acc = correct / max(total, 1)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def run_training_cycles(model: nn.Module, datamodule: SimpleImageDataModule, config: dict, device: torch.device):\n",
    "    history = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader = datamodule.train_dataloader()\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "\n",
    "    # Tahap 1: Feature Extraction (freeze backbone).\n",
    "    set_feature_extractor_grad(model.feature_extractor, freeze_until=\"all\")\n",
    "    optimizer_fe = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config[\"lr_feature_extraction\"], weight_decay=config[\"weight_decay\"])\n",
    "    for epoch in range(1, config[\"num_epochs_feature_extraction\"] + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer_fe, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        history.append({\n",
    "            \"stage\": \"feature_extraction\",\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"train_acc\": float(train_acc),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_acc\": float(val_acc),\n",
    "        })\n",
    "\n",
    "    # Tahap 2: Fine-tuning (opsional membuka layer backbone).\n",
    "    fine_tune_epochs = config.get(\"num_epochs_fine_tuning\", 0)\n",
    "    if fine_tune_epochs > 0:\n",
    "        set_feature_extractor_grad(model.feature_extractor, freeze_until=config.get(\"freeze_until\", \"all\"))\n",
    "        optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config[\"lr_fine_tuning\"], weight_decay=config[\"weight_decay\"])\n",
    "        for epoch in range(1, fine_tune_epochs + 1):\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer_ft, device)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "            history.append({\n",
    "                \"stage\": \"fine_tuning\",\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": float(train_loss),\n",
    "                \"train_acc\": float(train_acc),\n",
    "                \"val_loss\": float(val_loss),\n",
    "                \"val_acc\": float(val_acc),\n",
    "            })\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "history = run_training_cycles(model, datamodule, config, device)\n",
    "print(f\"Selesai training demo dengan {len(history)} entry riwayat.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca485dab",
   "metadata": {},
   "source": [
    "## I. Plot & Logging (Code)\n",
    "\n",
    "Visualisasi metrik dummy dan simpan artefak (plot serta ringkasan JSON) ke folder `outputs/` agar mudah diperiksa setelah praktikum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d817743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyusun history ke DataFrame untuk analisis cepat.\n",
    "history_df = pd.DataFrame(history)\n",
    "if not history_df.empty:\n",
    "    display(history_df)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    for stage in history_df[\"stage\"].unique():\n",
    "        subset = history_df[history_df[\"stage\"] == stage]\n",
    "        axes[0].plot(subset[\"epoch\"], subset[\"train_loss\"], marker=\"o\", label=f\"{stage} train\")\n",
    "        axes[0].plot(subset[\"epoch\"], subset[\"val_loss\"], marker=\"s\", label=f\"{stage} val\")\n",
    "        axes[1].plot(subset[\"epoch\"], subset[\"train_acc\"], marker=\"o\", label=f\"{stage} train\")\n",
    "        axes[1].plot(subset[\"epoch\"], subset[\"val_acc\"], marker=\"s\", label=f\"{stage} val\")\n",
    "    axes[0].set_title(\"Loss per Stage\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[1].set_title(\"Accuracy per Stage\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = project_root / config[\"fig_dir\"] / \"loss_accuracy_demo.png\"\n",
    "    fig.savefig(fig_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Plot tersimpan di: {fig_path}\")\n",
    "else:\n",
    "    print(\"History kosong: tidak ada data untuk divisualisasikan.\")\n",
    "\n",
    "summary = {\n",
    "    \"device\": str(device),\n",
    "    \"config\": config,\n",
    "    \"history\": history,\n",
    "}\n",
    "summary_path = project_root / config[\"log_dir\"] / \"run_summary.json\"\n",
    "with summary_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Ringkasan run tersimpan di: {summary_path}\")\n",
    "\n",
    "model_path = project_root / config[\"save_dir\"] / \"demo_model_state_dict.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model checkpoint dummy tersimpan di: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a6b836",
   "metadata": {},
   "source": [
    "## J. Ringkasan & Diskusi\n",
    "\n",
    "| Mode | Kecepatan | Kebutuhan Data | Risiko Overfitting | Kapan Dipilih |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Feature Extraction (Freeze) | Cepat (hanya train head) | Rendah | Rendah | Dataset kecil, domain mirip |\n",
    "| Fine-Tuning Parsial | Sedang (beberapa layer dibuka) | Menengah | Menengah | Saat butuh adaptasi moderat, layer akhir di-unfreeze |\n",
    "| Fine-Tuning Penuh | Paling lambat | Tinggi | Tinggi | Domain sangat berbeda, data cukup besar |\n",
    "\n",
    "Pertanyaan refleksi:\n",
    "- Jika domain target sangat berbeda, apakah freeze masih efektif?\n",
    "- Bagian mana yang kemungkinan besar perlu di-unfreeze duluan?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26fb0f2",
   "metadata": {},
   "source": [
    "## K. Checklist Praktikum\n",
    "\n",
    "- [ ] Pahami definisi & manfaat transfer learning.\n",
    "- [ ] Jelaskan perbedaan freeze vs fine-tune.\n",
    "- [ ] Ubah konfigurasi `freeze_until` dan jelaskan pengaruhnya pada jumlah parameter yang di-train.\n",
    "- [ ] Tunjukkan lokasi file output (`outputs/figures` & `outputs/reports`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0217280",
   "metadata": {},
   "source": [
    "## L. Referensi\n",
    "\n",
    "- Slide: **Deep Learning 06 — Transfer Learning (Tatap Muka)** — ringkasan definisi, manfaat, jenis-jenis, serta strategi fine-tuning vs freeze yang digunakan dalam praktikum ini.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
