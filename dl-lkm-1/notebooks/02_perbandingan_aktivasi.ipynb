{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9114fe47",
   "metadata": {},
   "source": [
    "# LKM 2 - Perbandingan Fungsi Aktivasi\n",
    "\n",
    "## Tujuan Pembelajaran\n",
    "- Memahami berbagai jenis fungsi aktivasi\n",
    "- Membandingkan karakteristik ReLU, Sigmoid, dan Tanh\n",
    "- Menganalisis kelebihan dan kekurangan masing-masing fungsi\n",
    "- Memvisualisasikan perbedaan dalam konteks neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cf030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ Library berhasil diimport!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65a795",
   "metadata": {},
   "source": [
    "## 1. Definisi Fungsi Aktivasi\n",
    "\n",
    "Mari kita definisikan berbagai fungsi aktivasi yang umum digunakan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbee0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi sesuai LKM\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: Rectified Linear Unit\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # clip untuk stabilitas numerik\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    \"\"\"Exponential Linear Unit\"\"\"\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def swish(x):\n",
    "    \"\"\"Swish activation function\"\"\"\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "# Test functions\n",
    "test_input = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"=== TEST FUNGSI AKTIVASI ===\")\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"ReLU: {relu(test_input)}\")\n",
    "print(f\"Sigmoid: {sigmoid(test_input)}\")\n",
    "print(f\"Tanh: {tanh(test_input)}\")\n",
    "print(f\"Leaky ReLU: {leaky_relu(test_input)}\")\n",
    "print(f\"ELU: {elu(test_input)}\")\n",
    "print(f\"Swish: {swish(test_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad81ba",
   "metadata": {},
   "source": [
    "## 2. Visualisasi Perbandingan Fungsi Aktivasi\n",
    "\n",
    "Sesuai dengan kode dalam LKM, mari kita buat visualisasi perbandingan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sesuai kode LKM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Definisi fungsi aktivasi\n",
    "relu_vals = lambda x: np.maximum(0, x)\n",
    "sigmoid_vals = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot sesuai LKM\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(z, relu_vals(z), label=\"ReLU\", linewidth=3, color='red')\n",
    "plt.plot(z, sigmoid_vals(z), label=\"Sigmoid\", linewidth=3, color='blue')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title(\"Perbandingan Fungsi Aktivasi (LKM)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Tambahan: Semua fungsi aktivasi\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(z, relu(z), label=\"ReLU\", linewidth=2.5)\n",
    "plt.plot(z, sigmoid(z), label=\"Sigmoid\", linewidth=2.5)\n",
    "plt.plot(z, tanh(z), label=\"Tanh\", linewidth=2.5)\n",
    "plt.plot(z, leaky_relu(z), label=\"Leaky ReLU\", linewidth=2.5)\n",
    "plt.legend()\n",
    "plt.title(\"Fungsi Aktivasi Utama\")\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom pada area kritis (-2 to 2)\n",
    "plt.subplot(2, 3, 3)\n",
    "z_zoom = np.linspace(-2, 2, 100)\n",
    "plt.plot(z_zoom, relu(z_zoom), label=\"ReLU\", linewidth=3)\n",
    "plt.plot(z_zoom, sigmoid(z_zoom), label=\"Sigmoid\", linewidth=3)\n",
    "plt.plot(z_zoom, tanh(z_zoom), label=\"Tanh\", linewidth=3)\n",
    "plt.legend()\n",
    "plt.title(\"Zoom Area Kritis (-2 to 2)\")\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Turunan fungsi aktivasi\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(z, sigmoid_derivative(z), label=\"Sigmoid'\", linewidth=2.5)\n",
    "plt.plot(z, tanh_derivative(z), label=\"Tanh'\", linewidth=2.5)\n",
    "plt.plot(z, relu_derivative(z), label=\"ReLU'\", linewidth=2.5)\n",
    "plt.legend()\n",
    "plt.title(\"Turunan Fungsi Aktivasi\")\n",
    "plt.xlabel('Input (z)')\n",
    "plt.ylabel('Derivative')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Perbandingan range\n",
    "plt.subplot(2, 3, 5)\n",
    "activation_ranges = {\n",
    "    'ReLU': '[0, ∞)',\n",
    "    'Sigmoid': '(0, 1)',\n",
    "    'Tanh': '(-1, 1)',\n",
    "    'Leaky ReLU': '(-∞, ∞)',\n",
    "    'ELU': '(-α, ∞)'\n",
    "}\n",
    "\n",
    "y_pos = np.arange(len(activation_ranges))\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "for i, (name, range_val) in enumerate(activation_ranges.items()):\n",
    "    plt.barh(i, 1, color=colors[i], alpha=0.7)\n",
    "    plt.text(0.5, i, f\"{name}: {range_val}\", ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.yticks(y_pos, list(activation_ranges.keys()))\n",
    "plt.xlabel('Range Comparison')\n",
    "plt.title('Output Ranges')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Karakteristik utama\n",
    "plt.subplot(2, 3, 6)\n",
    "characteristics = [\n",
    "    \"ReLU: Fast, No saturation (+), Dead neurons (-)\",\n",
    "    \"Sigmoid: Smooth, Probabilistic (+), Vanishing gradient (-)\", \n",
    "    \"Tanh: Zero-centered (+), Still saturates (-)\",\n",
    "    \"Leaky ReLU: No dead neurons (+), Small negative slope\",\n",
    "    \"ELU: Smooth, Negative values (+), Computational cost (-)\"\n",
    "]\n",
    "\n",
    "for i, char in enumerate(characteristics):\n",
    "    plt.text(0.05, 0.9 - i*0.18, char, fontsize=10, transform=plt.gca().transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=colors[i], alpha=0.3))\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.axis('off')\n",
    "plt.title('Karakteristik Utama')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214c48e",
   "metadata": {},
   "source": [
    "## 3. Analisis Mendalam Masing-masing Fungsi\n",
    "\n",
    "Mari kita analisis secara detail karakteristik masing-masing fungsi aktivasi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis detail setiap fungsi aktivasi\n",
    "\n",
    "def analyze_activation_function(func, name, x_range=(-5, 5)):\n",
    "    \"\"\"Analisis mendalam fungsi aktivasi\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 1000)\n",
    "    y = func(x)\n",
    "    \n",
    "    # Statistik dasar\n",
    "    min_val = np.min(y)\n",
    "    max_val = np.max(y)\n",
    "    mean_val = np.mean(y)\n",
    "    std_val = np.std(y)\n",
    "    \n",
    "    # Gradient (turunan numerik)\n",
    "    gradient = np.gradient(y, x)\n",
    "    max_gradient = np.max(gradient)\n",
    "    min_gradient = np.min(gradient)\n",
    "    \n",
    "    # Saturasi (daerah dengan gradient < 0.01)\n",
    "    saturation_points = np.sum(np.abs(gradient) < 0.01)\n",
    "    saturation_ratio = saturation_points / len(x)\n",
    "    \n",
    "    print(f\"\\n=== ANALISIS {name.upper()} ===\")\n",
    "    print(f\"Range nilai: [{min_val:.3f}, {max_val:.3f}]\")\n",
    "    print(f\"Mean: {mean_val:.3f}, Std: {std_val:.3f}\")\n",
    "    print(f\"Gradient range: [{min_gradient:.3f}, {max_gradient:.3f}]\")\n",
    "    print(f\"Rasio saturasi: {saturation_ratio:.1%}\")\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'max_grad': max_gradient,\n",
    "        'saturation': saturation_ratio\n",
    "    }\n",
    "\n",
    "# Analisis semua fungsi\n",
    "functions = [\n",
    "    (relu, \"ReLU\"),\n",
    "    (sigmoid, \"Sigmoid\"), \n",
    "    (tanh, \"Tanh\"),\n",
    "    (leaky_relu, \"Leaky ReLU\"),\n",
    "    (elu, \"ELU\")\n",
    "]\n",
    "\n",
    "analysis_results = []\n",
    "for func, name in functions:\n",
    "    result = analyze_activation_function(func, name)\n",
    "    analysis_results.append(result)\n",
    "\n",
    "# Buat tabel perbandingan\n",
    "df = pd.DataFrame(analysis_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                      TABEL PERBANDINGAN\")\n",
    "print(\"=\"*80)\n",
    "print(df.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e2351",
   "metadata": {},
   "source": [
    "## 4. Eksperimen: Pengaruh Fungsi Aktivasi pada Learning\n",
    "\n",
    "Mari kita lihat bagaimana berbagai fungsi aktivasi mempengaruhi proses pembelajaran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulasi simple learning dengan berbagai aktivasi\n",
    "\n",
    "class SimpleNeuron:\n",
    "    def __init__(self, activation_func, activation_name):\n",
    "        self.activation_func = activation_func\n",
    "        self.activation_name = activation_name\n",
    "        self.weight = np.random.normal(0, 0.1)\n",
    "        self.bias = np.random.normal(0, 0.1)\n",
    "        self.history = {'weights': [], 'outputs': [], 'gradients': []}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.weight * x + self.bias\n",
    "        a = self.activation_func(z)\n",
    "        \n",
    "        # Simpan untuk analisis\n",
    "        self.history['weights'].append(self.weight)\n",
    "        self.history['outputs'].append(a)\n",
    "        \n",
    "        return z, a\n",
    "    \n",
    "    def simulate_learning(self, x_values):\n",
    "        \"\"\"Simulasi proses learning\"\"\"\n",
    "        outputs = []\n",
    "        gradients = []\n",
    "        \n",
    "        for x in x_values:\n",
    "            z, a = self.forward(x)\n",
    "            outputs.append(a)\n",
    "            \n",
    "            # Simulasi gradient (turunan numerik sederhana)\n",
    "            eps = 1e-7\n",
    "            z_plus = self.weight * x + self.bias + eps\n",
    "            z_minus = self.weight * x + self.bias - eps\n",
    "            \n",
    "            grad = (self.activation_func(z_plus) - self.activation_func(z_minus)) / (2 * eps)\n",
    "            gradients.append(grad)\n",
    "            \n",
    "            # Update weight (simulasi SGD sederhana)\n",
    "            learning_rate = 0.01\n",
    "            self.weight += learning_rate * grad * 0.1  # update kecil\n",
    "        \n",
    "        return outputs, gradients\n",
    "\n",
    "# Test dengan berbagai input\n",
    "x_test = np.linspace(-3, 3, 50)\n",
    "\n",
    "# Buat neuron dengan berbagai aktivasi\n",
    "neurons = {\n",
    "    'ReLU': SimpleNeuron(relu, 'ReLU'),\n",
    "    'Sigmoid': SimpleNeuron(sigmoid, 'Sigmoid'),\n",
    "    'Tanh': SimpleNeuron(tanh, 'Tanh')\n",
    "}\n",
    "\n",
    "# Simulasi learning\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (name, neuron) in enumerate(neurons.items()):\n",
    "    outputs, gradients = neuron.simulate_learning(x_test)\n",
    "    \n",
    "    # Plot outputs\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.plot(x_test, outputs, label=f'{name} Output', linewidth=2)\n",
    "    plt.title(f'{name} - Outputs')\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot gradients\n",
    "    plt.subplot(2, 3, i+4)\n",
    "    plt.plot(x_test, gradients, label=f'{name} Gradient', linewidth=2, color='red')\n",
    "    plt.title(f'{name} - Gradients')\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Gradient')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Analisis gradient\n",
    "    avg_grad = np.mean(np.abs(gradients))\n",
    "    min_grad = np.min(gradients)\n",
    "    max_grad = np.max(gradients)\n",
    "    \n",
    "    print(f\"\\n{name} - Analisis Gradient:\")\n",
    "    print(f\"  Average |gradient|: {avg_grad:.4f}\")\n",
    "    print(f\"  Min gradient: {min_grad:.4f}\")\n",
    "    print(f\"  Max gradient: {max_grad:.4f}\")\n",
    "    print(f\"  Gradient variance: {np.var(gradients):.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6705",
   "metadata": {},
   "source": [
    "## 5. Kapan Menggunakan Fungsi Aktivasi Tertentu?\n",
    "\n",
    "Berikut adalah panduan pemilihan fungsi aktivasi berdasarkan konteks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a994fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panduan pemilihan fungsi aktivasi\n",
    "\n",
    "def create_activation_guide():\n",
    "    \"\"\"Membuat panduan pemilihan fungsi aktivasi\"\"\"\n",
    "    \n",
    "    guide_data = {\n",
    "        'Fungsi': ['ReLU', 'Sigmoid', 'Tanh', 'Leaky ReLU', 'ELU', 'Swish'],\n",
    "        'Best Use Case': [\n",
    "            'Hidden layers, CNN, general purpose',\n",
    "            'Binary classification output, probability',\n",
    "            'Hidden layers when zero-centered needed',\n",
    "            'When dead neurons are problem',\n",
    "            'When smooth negatives needed',\n",
    "            'Deep networks, recent architectures'\n",
    "        ],\n",
    "        'Pros': [\n",
    "            'Fast, no saturation, sparse activation',\n",
    "            'Smooth, probabilistic output',\n",
    "            'Zero-centered, stronger gradients than sigmoid',\n",
    "            'No dead neurons, allows negative values',\n",
    "            'Smooth, no sharp changes at zero',\n",
    "            'Self-gating, works well in practice'\n",
    "        ],\n",
    "        'Cons': [\n",
    "            'Dead neurons, not zero-centered',\n",
    "            'Vanishing gradient, saturated outputs',\n",
    "            'Still saturates, vanishing gradient',\n",
    "            'Small gradient for negatives',\n",
    "            'Computational cost, exponential',\n",
    "            'More complex, computational overhead'\n",
    "        ],\n",
    "        'Gradient Issues': [\n",
    "            'Zero gradient for negative inputs',\n",
    "            'Very small gradients at extremes',\n",
    "            'Small gradients at extremes',\n",
    "            'Small but non-zero negative gradients',\n",
    "            'Better gradient flow than ReLU',\n",
    "            'Generally good gradient properties'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_guide = pd.DataFrame(guide_data)\n",
    "    return df_guide\n",
    "\n",
    "guide = create_activation_guide()\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"                           PANDUAN PEMILIHAN FUNGSI AKTIVASI\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, row in guide.iterrows():\n",
    "    print(f\"\\n🔹 {row['Fungsi'].upper()}:\")\n",
    "    print(f\"   📋 Use Case: {row['Best Use Case']}\")\n",
    "    print(f\"   ✅ Pros: {row['Pros']}\")\n",
    "    print(f\"   ❌ Cons: {row['Cons']}\")\n",
    "    print(f\"   📈 Gradient: {row['Gradient Issues']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"💡 REKOMENDASI UMUM:\")\n",
    "print(\"   • Mulai dengan ReLU untuk hidden layers\")\n",
    "print(\"   • Gunakan Sigmoid untuk binary classification output\")\n",
    "print(\"   • Pertimbangkan Tanh jika perlu zero-centered\")\n",
    "print(\"   • Coba Leaky ReLU jika mengalami dead neurons\")\n",
    "print(\"   • Eksperimen dengan ELU/Swish untuk model kompleks\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73d937",
   "metadata": {},
   "source": [
    "## 6. Visualisasi Interaktif Decision Boundaries\n",
    "\n",
    "Mari kita lihat bagaimana berbagai fungsi aktivasi mempengaruhi decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi decision boundaries dengan berbagai aktivasi\n",
    "\n",
    "def compare_decision_boundaries():\n",
    "    \"\"\"Membandingkan decision boundaries dengan berbagai fungsi aktivasi\"\"\"\n",
    "    \n",
    "    # Setup data\n",
    "    x1, x2 = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "    inputs = np.stack([x1.flatten(), x2.flatten()], axis=1)\n",
    "    \n",
    "    # Parameter neuron\n",
    "    w = np.array([1, -0.5])\n",
    "    b = 0.2\n",
    "    \n",
    "    # Hitung z untuk semua input\n",
    "    z = inputs @ w + b\n",
    "    \n",
    "    # Apply berbagai fungsi aktivasi\n",
    "    activations = {\n",
    "        'ReLU': relu(z),\n",
    "        'Sigmoid': sigmoid(z),\n",
    "        'Tanh': tanh(z),\n",
    "        'Leaky ReLU': leaky_relu(z)\n",
    "    }\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (name, output) in enumerate(activations.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Reshape untuk plotting\n",
    "        output_2d = output.reshape(x1.shape)\n",
    "        \n",
    "        # Contour plot\n",
    "        contour = ax.contourf(x1, x2, output_2d, levels=20, alpha=0.7, cmap='RdYlBu')\n",
    "        \n",
    "        # Decision boundary (output = threshold)\n",
    "        if name == 'Sigmoid':\n",
    "            threshold = 0.5\n",
    "        elif name == 'Tanh':\n",
    "            threshold = 0.0\n",
    "        else:  # ReLU, Leaky ReLU\n",
    "            threshold = 0.5 * np.max(output)  # 50% of max\n",
    "        \n",
    "        ax.contour(x1, x2, output_2d, levels=[threshold], colors='black', linewidths=3)\n",
    "        \n",
    "        # Sample points\n",
    "        sample_points = np.array([[-2, -1], [-1, 2], [1, -1], [2, 2]])\n",
    "        for point in sample_points:\n",
    "            z_point = point @ w + b\n",
    "            if name == 'ReLU':\n",
    "                output_point = relu(z_point)\n",
    "            elif name == 'Sigmoid':\n",
    "                output_point = sigmoid(z_point)\n",
    "            elif name == 'Tanh':\n",
    "                output_point = tanh(z_point)\n",
    "            else:  # Leaky ReLU\n",
    "                output_point = leaky_relu(z_point)\n",
    "            \n",
    "            color = 'red' if output_point > threshold else 'blue'\n",
    "            ax.scatter(point[0], point[1], c=color, s=100, edgecolor='black', zorder=5)\n",
    "            ax.annotate(f'{output_point:.2f}', point, xytext=(5, 5), \n",
    "                       textcoords='offset points', fontsize=8, \n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.set_title(f'{name} Activation', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('x1')\n",
    "        ax.set_ylabel('x2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Colorbar\n",
    "        plt.colorbar(contour, ax=ax, label='Output Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analisis perbedaan\n",
    "    print(\"\\n📊 ANALISIS DECISION BOUNDARIES:\")\n",
    "    print(\"\\n🔹 ReLU:\")\n",
    "    print(\"   • Boundary tajam (step function)\")\n",
    "    print(\"   • Hanya region positif yang aktif\")\n",
    "    print(\"   • Suitable untuk sparse representations\")\n",
    "    \n",
    "    print(\"\\n🔹 Sigmoid:\")\n",
    "    print(\"   • Boundary halus dan gradual\")\n",
    "    print(\"   • Output probabilistic (0-1)\")\n",
    "    print(\"   • Good untuk binary classification\")\n",
    "    \n",
    "    print(\"\\n🔹 Tanh:\")\n",
    "    print(\"   • Boundary halus, zero-centered\")\n",
    "    print(\"   • Output range (-1, 1)\")\n",
    "    print(\"   • Better gradient flow than sigmoid\")\n",
    "    \n",
    "    print(\"\\n🔹 Leaky ReLU:\")\n",
    "    print(\"   • Boundary tajam dengan small negative slope\")\n",
    "    print(\"   • Prevents dead neurons\")\n",
    "    print(\"   • Maintains some information in negative region\")\n",
    "\n",
    "compare_decision_boundaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08506432",
   "metadata": {},
   "source": [
    "## 7. Kesimpulan dan Rekomendasi\n",
    "\n",
    "Berdasarkan semua analisis yang telah dilakukan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c66c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ringkasan dan kesimpulan final\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"              KESIMPULAN PERBANDINGAN FUNGSI AKTIVASI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conclusions = [\n",
    "    \"\\n🎯 TEMUAN UTAMA:\",\n",
    "    \"   1. ReLU adalah pilihan default yang baik untuk kebanyakan kasus\",\n",
    "    \"   2. Sigmoid ideal untuk output layer classification (probabilitas)\",\n",
    "    \"   3. Tanh memberikan gradients yang lebih kuat dibanding sigmoid\",\n",
    "    \"   4. Leaky ReLU mengatasi masalah dead neurons pada ReLU\",\n",
    "    \"   5. Pemilihan aktivasi bergantung pada arsitektur dan data\",\n",
    "    \n",
    "    \"\\n🔍 INSIGHT PENTING:\",\n",
    "    \"   • Vanishing gradient: Sigmoid & Tanh bermasalah di deep networks\",\n",
    "    \"   • Dead neurons: ReLU bisa 'mati' jika input selalu negatif\",\n",
    "    \"   • Zero-centering: Tanh lebih baik untuk hidden layers\",\n",
    "    \"   • Computational efficiency: ReLU paling cepat\",\n",
    "    \"   • Gradient flow: ReLU variants umumnya lebih baik\",\n",
    "    \n",
    "    \"\\n📋 REKOMENDASI PRAKTIS:\",\n",
    "    \"   🚀 START: Gunakan ReLU untuk hidden layers\",\n",
    "    \"   🎯 OUTPUT: Sigmoid (binary), Softmax (multiclass)\", \n",
    "    \"   🔧 TROUBLESHOOT: Leaky ReLU jika ada dead neurons\",\n",
    "    \"   ⚡ EXPERIMENT: ELU/Swish untuk fine-tuning performance\",\n",
    "    \"   📊 MONITOR: Selalu pantau gradient flow dan dead neurons\",\n",
    "    \n",
    "    \"\\n🎓 PEMBELAJARAN DARI LKM:\",\n",
    "    \"   • Fungsi aktivasi menentukan karakteristik output neuron\",\n",
    "    \"   • Pemilihan yang tepat crucial untuk performance model\",\n",
    "    \"   • Trade-off antara simplicity, speed, dan effectiveness\",\n",
    "    \"   • Visualisasi membantu memahami behavior function\",\n",
    "    \"   • Eksperimen empiris sering lebih baik dari teori\"\n",
    "]\n",
    "\n",
    "for conclusion in conclusions:\n",
    "    print(conclusion)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"💡 NEXT STEPS: Implementasi dalam neural networks dan evaluasi empiris!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate summary table untuk save\n",
    "summary_data = {\n",
    "    'Activation': ['ReLU', 'Sigmoid', 'Tanh', 'Leaky ReLU'],\n",
    "    'Range': ['[0, ∞)', '(0, 1)', '(-1, 1)', '(-∞, ∞)'],\n",
    "    'Best_Use': ['Hidden layers', 'Binary output', 'Hidden layers', 'Replace ReLU'],\n",
    "    'Pros': ['Fast, sparse', 'Probabilistic', 'Zero-centered', 'No dead neurons'],\n",
    "    'Cons': ['Dead neurons', 'Vanishing grad', 'Still saturates', 'Small neg grad']\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n📊 SUMMARY TABLE:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save hasil ke CSV untuk dokumentasi\n",
    "summary_df.to_csv('/home/juni/Praktikum/deep-learning/dl-lkm-1/results/activation_comparison.csv', index=False)\n",
    "print(f\"\\n✅ Summary saved to: results/activation_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
