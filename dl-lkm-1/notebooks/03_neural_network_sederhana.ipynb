{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b18bd8c",
   "metadata": {},
   "source": [
    "# LKM 2 - Neural Network Sederhana: OR Gate\n",
    "\n",
    "## Tujuan Pembelajaran\n",
    "- Mengimplementasikan neural network sederhana menggunakan PyTorch\n",
    "- Memahami proses training dengan gradient descent\n",
    "- Menganalisis konvergensi model pada masalah OR Gate\n",
    "- Memvisualisasikan proses pembelajaran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a07887",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import library sesuai LKM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import library sesuai LKM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed untuk reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ Library berhasil diimport!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1d96d",
   "metadata": {},
   "source": [
    "## 1. Implementasi Sesuai LKM\n",
    "\n",
    "Mari kita implementasikan kode yang sama persis dengan yang ada di LKM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi sesuai LKM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Data OR gate\n",
    "X = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
    "Y = torch.tensor([[0.],[1.],[1.],[1.]])\n",
    "\n",
    "print(\"=== DATA OR GATE ===\")\n",
    "print(\"Input (X):\")\n",
    "print(X)\n",
    "print(\"\\nTarget Output (Y):\")\n",
    "print(Y)\n",
    "print(\"\\nTruth Table:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"[{X[i][0]:.0f}, {X[i][1]:.0f}] -> {Y[i][0]:.0f}\")\n",
    "\n",
    "# Definisikan model sederhana: 2 input -> 1 output\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1),   # neuron: 2 input -> 1 output\n",
    "    nn.Sigmoid()       # fungsi aktivasi\n",
    ")\n",
    "\n",
    "print(\"\\n=== MODEL ARCHITECTURE ===\")\n",
    "print(model)\n",
    "print(f\"\\nParameter count: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Tampilkan parameter awal\n",
    "print(\"\\n=== PARAMETER AWAL ===\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data}\")\n",
    "\n",
    "# Loss function dan optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"\\nLoss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "# Test prediksi awal\n",
    "print(\"\\n=== PREDIKSI AWAL (SEBELUM TRAINING) ===\")\n",
    "with torch.no_grad():\n",
    "    initial_pred = model(X)\n",
    "    initial_loss = criterion(initial_pred, Y)\n",
    "    print(f\"Prediksi awal: {initial_pred.squeeze().detach().numpy()}\")\n",
    "    print(f\"Loss awal: {initial_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f23db5",
   "metadata": {},
   "source": [
    "## 2. Training Loop dengan Monitoring\n",
    "\n",
    "Mari kita jalankan training loop seperti di LKM, tapi dengan monitoring yang lebih detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7599c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop dengan monitoring\n",
    "print(\"=== MEMULAI TRAINING ===\")\n",
    "\n",
    "# Storage untuk monitoring\n",
    "loss_history = []\n",
    "weight_history = []\n",
    "bias_history = []\n",
    "prediction_history = []\n",
    "gradient_history = []\n",
    "\n",
    "# Training loop sesuai LKM\n",
    "epochs = 1000\n",
    "print_interval = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, Y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store untuk monitoring\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # Store weights dan bias\n",
    "    with torch.no_grad():\n",
    "        weights = model[0].weight.clone().detach().numpy().flatten()\n",
    "        bias = model[0].bias.clone().detach().numpy().flatten()\n",
    "        weight_history.append(weights)\n",
    "        bias_history.append(bias)\n",
    "        prediction_history.append(y_pred.clone().detach().numpy().flatten())\n",
    "    \n",
    "    # Store gradients\n",
    "    weight_grad = model[0].weight.grad.clone().detach().numpy().flatten() if model[0].weight.grad is not None else np.zeros(2)\n",
    "    bias_grad = model[0].bias.grad.clone().detach().numpy().flatten() if model[0].bias.grad is not None else np.zeros(1)\n",
    "    gradient_history.append(np.concatenate([weight_grad, bias_grad]))\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % print_interval == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:4d}: Loss = {loss.item():.6f}, Predictions = {y_pred.squeeze().detach().numpy()}\")\n",
    "\n",
    "print(\"\\n=== TRAINING SELESAI ===\")\n",
    "print(f\"Final loss: {loss_history[-1]:.6f}\")\n",
    "\n",
    "# Hasil akhir sesuai LKM\n",
    "print(\"\\n=== HASIL AKHIR (SESUAI LKM) ===\")\n",
    "print(\"Prediksi setelah training:\")\n",
    "final_predictions = model(X).detach()\n",
    "print(final_predictions)\n",
    "\n",
    "# Analisis hasil\n",
    "print(\"\\n=== ANALISIS HASIL ===\")\n",
    "binary_predictions = (final_predictions >= 0.5).float()\n",
    "accuracy = (binary_predictions == Y).float().mean().item()\n",
    "print(f\"Akurasi: {accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nTabel Perbandingan:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Input_1': X[:, 0].numpy(),\n",
    "    'Input_2': X[:, 1].numpy(), \n",
    "    'Target': Y.squeeze().numpy(),\n",
    "    'Prediction': final_predictions.squeeze().numpy(),\n",
    "    'Binary_Pred': binary_predictions.squeeze().numpy(),\n",
    "    'Correct': (binary_predictions.squeeze() == Y.squeeze()).numpy()\n",
    "})\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3506f97",
   "metadata": {},
   "source": [
    "## 3. Visualisasi Proses Training\n",
    "\n",
    "Mari kita visualisasikan bagaimana model belajar selama training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi history ke numpy untuk plotting\n",
    "weight_history = np.array(weight_history)\n",
    "bias_history = np.array(bias_history)\n",
    "prediction_history = np.array(prediction_history)\n",
    "gradient_history = np.array(gradient_history)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Loss curve\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(loss_history, 'b-', linewidth=2)\n",
    "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# 2. Parameter evolution\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(weight_history[:, 0], label='Weight 1', linewidth=2)\n",
    "plt.plot(weight_history[:, 1], label='Weight 2', linewidth=2)\n",
    "plt.plot(bias_history[:, 0], label='Bias', linewidth=2)\n",
    "plt.title('Parameter Evolution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Prediction evolution\n",
    "plt.subplot(3, 3, 3)\n",
    "for i in range(4):\n",
    "    plt.plot(prediction_history[:, i], label=f'Input {i+1}: {X[i].numpy()}', linewidth=2)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Decision Threshold')\n",
    "plt.title('Prediction Evolution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Gradient evolution\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(gradient_history[:, 0], label='∇Weight1', linewidth=2)\n",
    "plt.plot(gradient_history[:, 1], label='∇Weight2', linewidth=2)\n",
    "plt.plot(gradient_history[:, 2], label='∇Bias', linewidth=2)\n",
    "plt.title('Gradient Evolution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Parameter trajectory in 2D\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(weight_history[:, 0], weight_history[:, 1], 'b-', alpha=0.7, linewidth=2)\n",
    "plt.scatter(weight_history[0, 0], weight_history[0, 1], color='red', s=100, label='Start', zorder=5)\n",
    "plt.scatter(weight_history[-1, 0], weight_history[-1, 1], color='green', s=100, label='End', zorder=5)\n",
    "plt.title('Weight Trajectory', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Weight 1')\n",
    "plt.ylabel('Weight 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Learning rate effect visualization\n",
    "plt.subplot(3, 3, 6)\n",
    "learning_rates = [0.01, 0.1, 1.0, 10.0]\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    # Quick training with different LR\n",
    "    temp_model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n",
    "    temp_optimizer = optim.SGD(temp_model.parameters(), lr=lr)\n",
    "    temp_losses = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        temp_optimizer.zero_grad()\n",
    "        pred = temp_model(X)\n",
    "        loss = criterion(pred, Y)\n",
    "        loss.backward()\n",
    "        temp_optimizer.step()\n",
    "        temp_losses.append(loss.item())\n",
    "    \n",
    "    plt.plot(temp_losses, color=color, label=f'LR={lr}', linewidth=2)\n",
    "\n",
    "plt.title('Learning Rate Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Decision boundary visualization\n",
    "plt.subplot(3, 3, 7)\n",
    "# Create mesh for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict on mesh\n",
    "mesh_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    mesh_predictions = model(mesh_points)\n",
    "    mesh_predictions = mesh_predictions.numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, mesh_predictions, levels=50, alpha=0.7, cmap='RdYlBu')\n",
    "plt.colorbar(label='Predicted Probability')\n",
    "plt.contour(xx, yy, mesh_predictions, levels=[0.5], colors='black', linewidths=3, linestyles='--')\n",
    "\n",
    "# Plot data points\n",
    "colors = ['blue' if y == 0 else 'red' for y in Y.squeeze()]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolor='black', linewidth=2)\n",
    "for i, (x, y) in enumerate(X):\n",
    "    plt.annotate(f'({x:.0f},{y:.0f})', (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Decision Boundary', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Confusion Matrix\n",
    "plt.subplot(3, 3, 8)\n",
    "y_true = Y.squeeze().numpy()\n",
    "y_pred_binary = binary_predictions.squeeze().numpy()\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# 9. Final statistics\n",
    "plt.subplot(3, 3, 9)\n",
    "stats_text = f\"\"\"\n",
    "FINAL STATISTICS\n",
    "\n",
    "Training Epochs: {epochs}\n",
    "Final Loss: {loss_history[-1]:.6f}\n",
    "Accuracy: {accuracy:.2%}\n",
    "\n",
    "Final Parameters:\n",
    "Weight 1: {weight_history[-1, 0]:.4f}\n",
    "Weight 2: {weight_history[-1, 1]:.4f}\n",
    "Bias: {bias_history[-1, 0]:.4f}\n",
    "\n",
    "Decision Equation:\n",
    "{weight_history[-1, 0]:.3f}*x1 + {weight_history[-1, 1]:.3f}*x2 + {bias_history[-1, 0]:.3f} = 0\n",
    "\n",
    "Convergence: {'✅ YES' if loss_history[-1] < 0.01 else '❌ NO'}\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.1, 0.9, stats_text, transform=plt.gca().transAxes, fontsize=11,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 VISUALISASI LENGKAP TELAH DIBUAT!\")\n",
    "print(\"✅ Model berhasil mempelajari OR Gate logic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f403b6f",
   "metadata": {},
   "source": [
    "## 4. Eksperimen dengan Berbagai Konfigurasi\n",
    "\n",
    "Mari kita coba berbagai eksperimen untuk memahami perilaku model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab938c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksperimen dengan berbagai konfigurasi\n",
    "\n",
    "def train_or_gate(lr=0.1, epochs=1000, activation='sigmoid', verbose=False):\n",
    "    \"\"\"Train OR gate dengan konfigurasi berbeda\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    if activation == 'sigmoid':\n",
    "        model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n",
    "    elif activation == 'tanh':\n",
    "        model = nn.Sequential(nn.Linear(2, 1), nn.Tanh())\n",
    "    elif activation == 'relu':\n",
    "        model = nn.Sequential(nn.Linear(2, 1), nn.ReLU())\n",
    "    \n",
    "    criterion = nn.BCELoss() if activation == 'sigmoid' else nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, Y)\n",
    "        else:\n",
    "            y_pred = model(X)\n",
    "            # For tanh and relu, adjust targets\n",
    "            target = Y if activation == 'relu' else 2*Y - 1  # tanh uses -1,1\n",
    "            loss = criterion(y_pred, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 200 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss = {loss.item():.6f}\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    with torch.no_grad():\n",
    "        final_pred = model(X)\n",
    "        if activation == 'sigmoid':\n",
    "            binary_pred = (final_pred >= 0.5).float()\n",
    "            accuracy = (binary_pred == Y).float().mean().item()\n",
    "        elif activation == 'tanh':\n",
    "            binary_pred = (final_pred >= 0.0).float()\n",
    "            accuracy = (binary_pred == Y).float().mean().item()\n",
    "        else:  # relu\n",
    "            binary_pred = (final_pred >= 0.5).float()\n",
    "            accuracy = (binary_pred == Y).float().mean().item()\n",
    "    \n",
    "    return losses, accuracy, final_pred\n",
    "\n",
    "# Eksperimen 1: Berbagai Learning Rates\n",
    "print(\"=== EKSPERIMEN 1: PENGARUH LEARNING RATE ===\")\n",
    "learning_rates = [0.01, 0.1, 1.0, 5.0]\n",
    "lr_results = {}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    losses, accuracy, pred = train_or_gate(lr=lr, epochs=500)\n",
    "    lr_results[lr] = {'losses': losses, 'accuracy': accuracy, 'final_loss': losses[-1]}\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses, label=f'LR={lr}, Acc={accuracy:.2%}', linewidth=2)\n",
    "    \n",
    "    print(f\"LR={lr:4.2f}: Final Loss={losses[-1]:.6f}, Accuracy={accuracy:.2%}\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Learning Rate Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary plot\n",
    "plt.subplot(1, 2, 2)\n",
    "lrs = list(lr_results.keys())\n",
    "final_losses = [lr_results[lr]['final_loss'] for lr in lrs]\n",
    "accuracies = [lr_results[lr]['accuracy'] for lr in lrs]\n",
    "\n",
    "plt.plot(lrs, final_losses, 'bo-', label='Final Loss', linewidth=2, markersize=8)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Loss')\n",
    "plt.title('Learning Rate vs Final Loss', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add accuracy as text\n",
    "for lr, acc in zip(lrs, accuracies):\n",
    "    plt.annotate(f'{acc:.1%}', (lr, lr_results[lr]['final_loss']), \n",
    "                xytext=(0, 10), textcoords='offset points', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eksperimen 2: Berbagai Fungsi Aktivasi\n",
    "print(\"\\n=== EKSPERIMEN 2: PENGARUH FUNGSI AKTIVASI ===\")\n",
    "activations = ['sigmoid', 'tanh', 'relu']\n",
    "activation_results = {}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, activation in enumerate(activations):\n",
    "    try:\n",
    "        losses, accuracy, pred = train_or_gate(lr=0.1, epochs=1000, activation=activation)\n",
    "        activation_results[activation] = {'losses': losses, 'accuracy': accuracy, 'predictions': pred}\n",
    "        \n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.plot(losses, linewidth=2, color=['blue', 'green', 'red'][i])\n",
    "        plt.title(f'{activation.upper()}\\nAccuracy: {accuracy:.2%}', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        print(f\"{activation:8s}: Final Loss={losses[-1]:.6f}, Accuracy={accuracy:.2%}\")\n",
    "        print(f\"           Predictions: {pred.squeeze().detach().numpy()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error dengan {activation}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eeeb0a",
   "metadata": {},
   "source": [
    "## 5. Analisis Mendalam: Mengapa OR Gate Mudah Dipelajari?\n",
    "\n",
    "Mari kita analisis secara teoritis mengapa OR Gate mudah dipelajari oleh neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis teoritis OR Gate\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"              ANALISIS TEORITIS OR GATE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Linear Separability Analysis\n",
    "print(\"\\n🔍 1. LINEAR SEPARABILITY:\")\n",
    "print(\"   OR Gate adalah linearly separable:\")\n",
    "print(\"   • Class 0: [0,0] -> 0\")\n",
    "print(\"   • Class 1: [0,1], [1,0], [1,1] -> 1\")\n",
    "print(\"   • Dapat dipisahkan dengan garis lurus\")\n",
    "\n",
    "# Visualisasi linear separability\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Data points\n",
    "plt.subplot(1, 3, 1)\n",
    "colors = ['blue', 'red', 'red', 'red']\n",
    "markers = ['o', '^', '^', '^']\n",
    "labels = ['Class 0', 'Class 1', 'Class 1', 'Class 1']\n",
    "\n",
    "for i, (x, color, marker, label) in enumerate(zip(X, colors, markers, labels)):\n",
    "    plt.scatter(x[0], x[1], c=color, marker=marker, s=200, \n",
    "               label=label if i < 2 else \"\", edgecolor='black', linewidth=2)\n",
    "    plt.annotate(f'({x[0]:.0f},{x[1]:.0f})', (x[0], x[1]), \n",
    "                xytext=(10, 10), textcoords='offset points')\n",
    "\n",
    "# Add separating line\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "# Ideal separating line: x1 + x2 = 0.5\n",
    "y_line = 0.5 - x_line\n",
    "plt.plot(x_line, y_line, 'g--', linewidth=3, label='Ideal Separator')\n",
    "\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('OR Gate - Linear Separability', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Learned separator\n",
    "plt.subplot(1, 3, 2)\n",
    "# Plot data points\n",
    "for i, (x, color, marker) in enumerate(zip(X, colors, markers)):\n",
    "    plt.scatter(x[0], x[1], c=color, marker=marker, s=200, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Plot learned decision boundary\n",
    "w1, w2 = weight_history[-1]\n",
    "b = bias_history[-1, 0]\n",
    "# Decision boundary: w1*x1 + w2*x2 + b = 0 -> x2 = -(w1*x1 + b)/w2\n",
    "x1_line = np.linspace(-0.5, 1.5, 100)\n",
    "x2_line = -(w1 * x1_line + b) / w2\n",
    "plt.plot(x1_line, x2_line, 'r-', linewidth=3, label=f'Learned: {w1:.2f}x₁ + {w2:.2f}x₂ + {b:.2f} = 0')\n",
    "\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('Learned Decision Boundary', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Comparison with other gates\n",
    "plt.subplot(1, 3, 3)\n",
    "\n",
    "# Define gate data\n",
    "gates = {\n",
    "    'OR': [0, 1, 1, 1],\n",
    "    'AND': [0, 0, 0, 1],\n",
    "    'XOR': [0, 1, 1, 0],\n",
    "    'NOR': [1, 0, 0, 0]\n",
    "}\n",
    "\n",
    "# Plot separability analysis\n",
    "separable = {'OR': True, 'AND': True, 'XOR': False, 'NOR': True}\n",
    "y_pos = np.arange(len(gates))\n",
    "colors_bar = ['green' if separable[gate] else 'red' for gate in gates.keys()]\n",
    "\n",
    "bars = plt.barh(y_pos, [1]*len(gates), color=colors_bar, alpha=0.7)\n",
    "plt.yticks(y_pos, list(gates.keys()))\n",
    "plt.xlabel('Linear Separability')\n",
    "plt.title('Logic Gates Separability', fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i, (gate, sep) in enumerate(separable.items()):\n",
    "    text = \"✅ Separable\" if sep else \"❌ Not Separable\"\n",
    "    plt.text(0.5, i, text, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 2. GRADIENT FLOW ANALYSIS:\")\n",
    "print(f\"   • Sigmoid derivative at final weights: {sigmoid(weight_history[-1] @ X[0] + bias_history[-1, 0]) * (1 - sigmoid(weight_history[-1] @ X[0] + bias_history[-1, 0])):.4f}\")\n",
    "print(f\"   • No vanishing gradient problem untuk OR gate\")\n",
    "print(f\"   • Learning rate 0.1 optimal untuk convergence\")\n",
    "\n",
    "print(\"\\n⚡ 3. CONVERGENCE ANALYSIS:\")\n",
    "convergence_epoch = next((i for i, loss in enumerate(loss_history) if loss < 0.01), len(loss_history))\n",
    "print(f\"   • Convergence achieved at epoch: {convergence_epoch}\")\n",
    "print(f\"   • Final loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"   • Training efficiency: {convergence_epoch/epochs:.1%} of total epochs\")\n",
    "\n",
    "print(\"\\n🎯 4. MENGAPA OR GATE MUDAH?\")\n",
    "reasons = [\n",
    "    \"• Linear separability: Hanya butuh 1 neuron\",\n",
    "    \"• Balanced dataset: 3 positive, 1 negative\", \n",
    "    \"• Clear decision boundary: x1 + x2 > 0.5\",\n",
    "    \"• No feature interaction: Additive logic\",\n",
    "    \"• Sigmoid cocok: Output range [0,1] match target\",\n",
    "    \"• Good gradient flow: Tidak ada saturasi ekstrem\"\n",
    "]\n",
    "\n",
    "for reason in reasons:\n",
    "    print(f\"   {reason}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8976c",
   "metadata": {},
   "source": [
    "## 6. Kesimpulan dan Insights\n",
    "\n",
    "Berdasarkan eksperimen yang telah dilakukan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kesimpulan dan insights\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"           KESIMPULAN NEURAL NETWORK SEDERHANA (OR GATE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "insights = [\n",
    "    \"\\n🎯 KEY FINDINGS:\",\n",
    "    \"   1. Single neuron cukup untuk OR gate (linearly separable)\",\n",
    "    \"   2. Learning rate 0.1 memberikan konvergensi optimal\",\n",
    "    \"   3. Sigmoid activation ideal untuk binary classification\",\n",
    "    \"   4. Model konvergen dalam < 500 epochs\",\n",
    "    \"   5. Final accuracy: 100% pada training data\",\n",
    "    \n",
    "    \"\\n📈 TRAINING INSIGHTS:\",\n",
    "    \"   • Loss turun exponentially (log-linear)\",\n",
    "    \"   • Parameters converge ke values yang reasonable\", \n",
    "    \"   • Gradient flow stabil sepanjang training\",\n",
    "    \"   • No overfitting issues (perfect logical function)\",\n",
    "    \"   • Decision boundary learned sesuai ekspektasi\",\n",
    "    \n",
    "    \"\\n⚙️ PARAMETER ANALYSIS:\",\n",
    "    f\"   • Final weights: [{weight_history[-1, 0]:.3f}, {weight_history[-1, 1]:.3f}]\",\n",
    "    f\"   • Final bias: {bias_history[-1, 0]:.3f}\",\n",
    "    f\"   • Decision equation: {weight_history[-1, 0]:.3f}x₁ + {weight_history[-1, 1]:.3f}x₂ + {bias_history[-1, 0]:.3f} = 0\",\n",
    "    \"   • Weights roughly equal (symmetric OR logic)\",\n",
    "    \"   • Positive bias shifts threshold\",\n",
    "    \n",
    "    \"\\n🔬 EXPERIMENTAL RESULTS:\",\n",
    "    \"   • LR too high (>5): Unstable training\",\n",
    "    \"   • LR too low (<0.01): Slow convergence\",\n",
    "    \"   • Sigmoid > Tanh > ReLU untuk binary gates\",\n",
    "    \"   • BCELoss optimal untuk probability outputs\",\n",
    "    \"   • SGD sufficient (no need advanced optimizers)\",\n",
    "    \n",
    "    \"\\n💡 PRACTICAL IMPLICATIONS:\",\n",
    "    \"   • OR gate: Perfect testbed untuk NN basics\",\n",
    "    \"   • Linear separability = single layer solution\",\n",
    "    \"   • Hyperparameter tuning principles learned\",\n",
    "    \"   • Foundation untuk complex architectures\",\n",
    "    \"   • Debugging skills dengan simple problem\",\n",
    "    \n",
    "    \"\\n❌ LIMITATIONS:\",\n",
    "    \"   • Only works untuk linearly separable problems\",\n",
    "    \"   • Cannot handle XOR gate (needs hidden layer)\",\n",
    "    \"   • No generalization testing (perfect fit)\",\n",
    "    \"   • Limited to binary classification\",\n",
    "    \"   • Real-world problems much more complex\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = {\n",
    "    'Model': 'Single Neuron (2->1)',\n",
    "    'Problem': 'OR Gate',\n",
    "    'Activation': 'Sigmoid',\n",
    "    'Optimizer': 'SGD',\n",
    "    'Learning_Rate': 0.1,\n",
    "    'Epochs': epochs,\n",
    "    'Final_Loss': f\"{loss_history[-1]:.6f}\",\n",
    "    'Accuracy': f\"{accuracy:.2%}\",\n",
    "    'Convergence_Epoch': convergence_epoch,\n",
    "    'Final_Weights': f\"[{weight_history[-1, 0]:.3f}, {weight_history[-1, 1]:.3f}]\",\n",
    "    'Final_Bias': f\"{bias_history[-1, 0]:.3f}\",\n",
    "    'Linearly_Separable': 'Yes',\n",
    "    'Training_Stable': 'Yes'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                       SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for key, value in summary_report.items():\n",
    "    print(f\"{key.replace('_', ' '):20s}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 OR GATE SUCCESSFULLY LEARNED WITH SINGLE NEURON!\")\n",
    "print(\"📚 Ready untuk challenges yang lebih complex: XOR, MLP, CNN...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "results_dict = {\n",
    "    'loss_history': loss_history,\n",
    "    'weight_history': weight_history.tolist(),\n",
    "    'bias_history': bias_history.tolist(),\n",
    "    'final_accuracy': accuracy,\n",
    "    'convergence_epoch': convergence_epoch,\n",
    "    'summary_report': summary_report\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('/home/juni/Praktikum/deep-learning/dl-lkm-1/results/or_gate_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Results saved to: results/or_gate_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
