{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Fine-Tuning for Stock Prediction (Dummy Data)\n",
    "\n",
    "Notebook ini membangun baseline model LSTM dan skenario fine-tuning menggunakan data saham dummy yang realistis. Fokus: pipeline yang terstruktur, reproducible, dan evaluasi yang jelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Local imports\n",
    "sys.path.append(os.getcwd())\n",
    "from src.data_generator import generate_stock_data, create_time_series_features\n",
    "from src.utils import calculate_metrics, plot_stock_predictions, plot_training_history, save_model_summary\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SEED = 42\n",
    "SOURCE_SEED = 101\n",
    "TARGET_SEED = 202\n",
    "\n",
    "SEQ_LEN = 60\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_BASELINE = 25\n",
    "EPOCHS_PRETRAIN = 30\n",
    "EPOCHS_FT_HEAD = 15\n",
    "EPOCHS_FT_FULL = 10\n",
    "\n",
    "LR_BASE = 1e-3\n",
    "LR_FT = 1e-4\n",
    "LR_FT_FULL = 5e-5\n",
    "\n",
    "N_DAYS_SOURCE = 1500\n",
    "N_DAYS_TARGET = 1000\n",
    "SOURCE_START_PRICE = 100.0\n",
    "TARGET_START_PRICE = 60.0\n",
    "SOURCE_VOL = 0.015\n",
    "TARGET_VOL = 0.025\n",
    "\n",
    "TARGET_COL = 'Close'\n",
    "\n",
    "OUT_DIR = 'notebooks'\n",
    "MODEL_PATH = os.path.join(OUT_DIR, 'best_lstm_finetune_model.keras')\n",
    "SUMMARY_PATH = os.path.join(OUT_DIR, 'best_lstm_finetune_model.txt')\n",
    "METRICS_PATH = os.path.join(OUT_DIR, 'lstm_finetune_metrics.csv')\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print('Seeds set. Output dir:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dummy Data + Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating source/target datasets...')\n",
    "src_df = generate_stock_data(n_days=N_DAYS_SOURCE, start_price=SOURCE_START_PRICE, volatility=SOURCE_VOL, seed=SOURCE_SEED)\n",
    "tgt_df = generate_stock_data(n_days=N_DAYS_TARGET, start_price=TARGET_START_PRICE, volatility=TARGET_VOL, seed=TARGET_SEED)\n",
    "\n",
    "src_df = create_time_series_features(src_df)\n",
    "tgt_df = create_time_series_features(tgt_df)\n",
    "\n",
    "def clean_features(df):\n",
    "    df = df.copy()\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    df = df[numeric_cols].dropna()\n",
    "    return df\n",
    "\n",
    "src_df_clean = clean_features(src_df)\n",
    "tgt_df_clean = clean_features(tgt_df)\n",
    "\n",
    "print(f'Source rows: {len(src_df_clean)} | Target rows: {len(tgt_df_clean)}')\n",
    "print('Feature columns:', list(src_df_clean.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Source Close range: {src_df_clean['Close'].min():.2f} - {src_df_clean['Close'].max():.2f}\")\n",
    "print(f\"Target Close range: {tgt_df_clean['Close'].min():.2f} - {tgt_df_clean['Close'].max():.2f}\")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,4))\n",
    "ax[0].plot(src_df_clean['Close'].values); ax[0].set_title('Source Close'); ax[0].grid(True)\n",
    "ax[1].plot(tgt_df_clean['Close'].values); ax[1].set_title('Target Close'); ax[1].grid(True)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sequences (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences_scaled(df, target_col, seq_len, train_frac=0.7, val_frac=0.15):\n",
    "    arr = df.values.astype('float32')\n",
    "    cols = df.columns.tolist()\n",
    "    target_idx = cols.index(target_col)\n",
    "    n = len(arr)\n",
    "    train_end = int(n * train_frac)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(arr[:train_end])\n",
    "    scaled = scaler.transform(arr)\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, n):\n",
    "        X.append(scaled[i-seq_len:i])\n",
    "        y.append(scaled[i, target_idx])\n",
    "    X = np.array(X, dtype='float32')\n",
    "    y = np.array(y, dtype='float32')\n",
    "    N = len(X)\n",
    "    t_train = int(N * train_frac)\n",
    "    t_val = int(N * val_frac)\n",
    "    X_train, y_train = X[:t_train], y[:t_train]\n",
    "    X_val, y_val = X[t_train:t_train+t_val], y[t_train:t_train+t_val]\n",
    "    X_test, y_test = X[t_train+t_val:], y[t_train+t_val:]\n",
    "    return (X_train, y_train, X_val, y_val, X_test, y_test, scaler, target_idx, cols)\n",
    "\n",
    "def inverse_scale_target(scaler, y_scaled, target_idx, n_features):\n",
    "    y_scaled = np.asarray(y_scaled).reshape(-1, 1)\n",
    "    zeros = np.zeros((len(y_scaled), n_features), dtype='float32')\n",
    "    zeros[:, target_idx] = y_scaled[:, 0]\n",
    "    inv = scaler.inverse_transform(zeros)\n",
    "    return inv[:, target_idx]\n",
    "\n",
    "def direction_accuracy(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    dy_true = np.diff(y_true)\n",
    "    dy_pred = np.diff(y_pred)\n",
    "    return (np.mean(np.sign(dy_true) == np.sign(dy_pred)) * 100.0)\n",
    "\n",
    "src_splits = build_sequences_scaled(src_df_clean, TARGET_COL, SEQ_LEN)\n",
    "tgt_splits = build_sequences_scaled(tgt_df_clean, TARGET_COL, SEQ_LEN)\n",
    "\n",
    "(X_src_tr, y_src_tr, X_src_val, y_src_val, X_src_te, y_src_te, scaler_src, tgt_idx_src, cols_src) = src_splits\n",
    "(X_tgt_tr, y_tgt_tr, X_tgt_val, y_tgt_val, X_tgt_te, y_tgt_te, scaler_tgt, tgt_idx_tgt, cols_tgt) = tgt_splits\n",
    "\n",
    "n_features = X_tgt_tr.shape[2]; seq_len = X_tgt_tr.shape[1]\n",
    "n_features, seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(n_features, seq_len, lstm_units=(128, 64), dense_units=64, dropout=0.2, lr=1e-3):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units[0], return_sequences=True, dropout=dropout, recurrent_dropout=dropout, input_shape=(seq_len, n_features), name='lstm_1'))\n",
    "    model.add(BatchNormalization(name='bn_1'))\n",
    "    model.add(LSTM(lstm_units[1], dropout=dropout, recurrent_dropout=dropout, name='lstm_2'))\n",
    "    model.add(Dense(dense_units, activation='relu', name='dense_1'))\n",
    "    model.add(Dropout(dropout, name='dropout_1'))\n",
    "    model.add(Dense(1, activation='linear', name='output'))\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def compile_callbacks():\n",
    "    return [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    ]\n",
    "\n",
    "print('Model builder ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Train-from-scratch on Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = build_lstm(n_features, seq_len, lr=LR_BASE)\n",
    "history_base = model_base.fit(\n",
    "    X_tgt_tr, y_tgt_tr,\n",
    "    validation_data=(X_tgt_val, y_tgt_val),\n",
    "    epochs=EPOCHS_BASELINE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=compile_callbacks(),\n",
    "    verbose=1\n",
    ")\n",
    "plot_training_history(history_base)\n",
    "\n",
    "y_pred_base_scaled = model_base.predict(X_tgt_te, verbose=0).reshape(-1)\n",
    "y_test_scaled = y_tgt_te.reshape(-1)\n",
    "y_pred_base = inverse_scale_target(scaler_tgt, y_pred_base_scaled, tgt_idx_tgt, n_features)\n",
    "y_test = inverse_scale_target(scaler_tgt, y_test_scaled, tgt_idx_tgt, n_features)\n",
    "\n",
    "metrics_base = calculate_metrics(y_test, y_pred_base)\n",
    "metrics_base['Direction_Accuracy'] = direction_accuracy(y_test, y_pred_base)\n",
    "print('Baseline metrics:', metrics_base)\n",
    "plot_stock_predictions(y_test, y_pred_base, title='Baseline: Actual vs Predicted (Target)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain on Source + Fine-Tune on Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = build_lstm(n_features, seq_len, lr=LR_BASE)\n",
    "history_pre = model_ft.fit(\n",
    "    X_src_tr, y_src_tr,\n",
    "    validation_data=(X_src_val, y_src_val),\n",
    "    epochs=EPOCHS_PRETRAIN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=compile_callbacks(),\n",
    "    verbose=1\n",
    ")\n",
    "plot_training_history(history_pre)\n",
    "\n",
    "# Freeze lower layers for head fine-tune\n",
    "for layer in model_ft.layers:\n",
    "    if layer.name in ('lstm_1', 'bn_1'):\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "model_ft.compile(optimizer=Adam(learning_rate=LR_FT), loss='mse', metrics=['mae'])\n",
    "\n",
    "history_ft_head = model_ft.fit(\n",
    "    X_tgt_tr, y_tgt_tr,\n",
    "    validation_data=(X_tgt_val, y_tgt_val),\n",
    "    epochs=EPOCHS_FT_HEAD,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=compile_callbacks(),\n",
    "    verbose=1\n",
    ")\n",
    "plot_training_history(history_ft_head)\n",
    "\n",
    "# Unfreeze all for short full fine-tune\n",
    "for layer in model_ft.layers:\n",
    "    layer.trainable = True\n",
    "model_ft.compile(optimizer=Adam(learning_rate=LR_FT_FULL), loss='mse', metrics=['mae'])\n",
    "history_ft_full = model_ft.fit(\n",
    "    X_tgt_tr, y_tgt_tr,\n",
    "    validation_data=(X_tgt_val, y_tgt_val),\n",
    "    epochs=EPOCHS_FT_FULL,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=compile_callbacks(),\n",
    "    verbose=1\n",
    ")\n",
    "plot_training_history(history_ft_full)\n",
    "\n",
    "# Evaluate on test\n",
    "y_pred_ft_scaled = model_ft.predict(X_tgt_te, verbose=0).reshape(-1)\n",
    "y_pred_ft = inverse_scale_target(scaler_tgt, y_pred_ft_scaled, tgt_idx_tgt, n_features)\n",
    "metrics_ft = calculate_metrics(y_test, y_pred_ft)\n",
    "metrics_ft['Direction_Accuracy'] = direction_accuracy(y_test, y_pred_ft)\n",
    "print('Fine-tune metrics:', metrics_ft)\n",
    "plot_stock_predictions(y_test, y_pred_ft, title='Fine-tuned: Actual vs Predicted (Target)')\n",
    "\n",
    "# Save artifacts\n",
    "model_ft.save(MODEL_PATH)\n",
    "save_model_summary(model_ft, SUMMARY_PATH)\n",
    "print('Saved model to:', MODEL_PATH)\n",
    "print('Saved summary to:', SUMMARY_PATH)\n",
    "\n",
    "# Compare metrics\n",
    "df_metrics = pd.DataFrame([\n",
    "    {'model': 'baseline', **metrics_base},\n",
    "    {'model': 'fine_tune', **metrics_ft},\n",
    "])\n",
    "df_metrics.to_csv(METRICS_PATH, index=False)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Forecast (Next 5 Days, Target Domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_n_steps(model, last_seq, n_steps, target_idx):\n",
    "    seq = last_seq.copy()\n",
    "    preds = []\n",
    "    for _ in range(n_steps):\n",
    "        yhat = model.predict(seq[np.newaxis, ...], verbose=0)[0, 0]\n",
    "        preds.append(yhat)\n",
    "        new_row = seq[-1].copy()\n",
    "        new_row[target_idx] = yhat\n",
    "        seq = np.vstack([seq[1:], new_row])\n",
    "    return np.array(preds)\n",
    "\n",
    "last_seq = X_tgt_te[-1]  # scaled space\n",
    "preds_scaled = forecast_n_steps(model_ft, last_seq, n_steps=5, target_idx=tgt_idx_tgt)\n",
    "preds_unscaled = inverse_scale_target(scaler_tgt, preds_scaled, tgt_idx_tgt, n_features)\n",
    "print('Next 5-day forecast (target units):', np.round(preds_unscaled, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

